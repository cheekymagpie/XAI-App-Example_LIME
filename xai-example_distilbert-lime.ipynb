{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Install Python3 (full version, this is important)\n",
    "sudo apt-get install python3-full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Double-check Python3 version\n",
    "python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Double-check that tkinter is available (no error means it is available)\n",
    "python3 -c \"import tkinter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Set up virtual environment (otherwise apt and pip will conflict because of PEP668)\n",
    "python3 -m venv .venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Install Git\n",
    "sudo apt-get install git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Clone my repo to current location (and hope for no rate limit error)\n",
    "git clone https://github.com/cheekymagpie/XAI-App-Example_LIME.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Start virtual environment\n",
    "source .venv/bin/activate\n",
    "# You should now see (.venv) in front of terminal lines, indicating that the virtual environment is active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Install requirements WITHOUT CACHING (important, otherwise disk space will be gone in an instant)\n",
    "# Pray it all installs fine\n",
    "pip install --no-cache-dir torch transformers lime matplotlib psutil gputil\n",
    "\n",
    "# Could also be done via requirements.txt but not sure if no-cache-dir works for it?\n",
    "# pip install -r --no-cache-dir requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# If you are able to have a GUI on your device, you can simply run the script\n",
    "\n",
    "# cd into cloned repo folder first\n",
    "cd XAI-App-Example_LIME\n",
    "# run script and pray\n",
    "python3 xai-example_distilbert-lime.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, if you're only able to use the notebook, this is the script modified to be notebook-only\n",
    "# (or if Github rate limit error strikes)\n",
    "\n",
    "# Import PyTorch for tensor functionality\n",
    "import torch\n",
    "# Import the PyTorch quantization submodule for resource usage optimization (might not be needed if enough resources are available)\n",
    "import torch.quantization\n",
    "# Import the text explainer from LIME\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "# Import Huggingface Transformers library for loading the model\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# Import signal and time for \"emergency termination\" functionality (might not be needed if enough resources are available)\n",
    "import signal\n",
    "import time\n",
    "# Import matplotlib for plotting functionality\n",
    "import matplotlib.pyplot as plt\n",
    "# Import necessary library to display results in the notebook only\n",
    "from IPython.display import display, clear_output\n",
    "# Import necessary libraries for performance monitoring in its own thread\n",
    "import threading\n",
    "import psutil\n",
    "import GPUtil\n",
    "\n",
    "start_time = time.time()\n",
    "# Returns the current time in seconds since the epoch (Jan 1, 1970?)\n",
    "# By calling this function before and after the code, we calculate the elapsed time (execution time of the script)\n",
    "\n",
    "# Defining a global flag used for stopping the separate monitoring thread when the main script gets stopped\n",
    "stop_monitoring = False\n",
    "# Setting up resource monitoring functionality\n",
    "def monitor_resources():\n",
    "    global stop_monitoring\n",
    "    while not stop_monitoring:\n",
    "        # For CPU usage measuring\n",
    "        cpu_usage = psutil.cpu_percent()\n",
    "        # For RAM usage measuring\n",
    "        ram_usage = psutil.virtual_memory().percent\n",
    "        # For GPU usage measuring\n",
    "        gpus = GPUtil.getGPUs()\n",
    "        # Check whether one or more GPUs are available\n",
    "        if gpus:\n",
    "            gpu = gpus[0]\n",
    "            gpu_load = gpu.load\n",
    "            gpu_memory_usage = gpu.memoryUtil\n",
    "\n",
    "            print(f\"CPU Usage: {cpu_usage} %\")\n",
    "            print(f\"RAM Usage: {ram_usage} %\")\n",
    "            print(f\"GPU Load: {gpu_load * 100} %\")\n",
    "            print(f\"GPU Memory Usage: {gpu_memory_usage * 100} %\")\n",
    "        else:\n",
    "            print(\"No GPU available.\")\n",
    "\n",
    "        # Sleep for 300 seconds (5 minutes); we want to print resource monitoring output every 5 minutes\n",
    "        time.sleep(300)\n",
    "\n",
    "# Starting resource monitoring in a separate thread\n",
    "monitor_thread = threading.Thread(target=monitor_resources)\n",
    "monitor_thread.start()        \n",
    "\n",
    "# Setting up a function for getting user input from a simple GUI\n",
    "def run_model():\n",
    "    # Get the user's input from the text field (modified for in-notebook usage)\n",
    "    user_input = input(\"Please enter a text sequence for the model to classify: \")\n",
    "    if user_input:\n",
    "        try:\n",
    "            # Setting up the \"emergency termination\" handler\n",
    "            # Define the handler\n",
    "            def handler(signum, frame):\n",
    "                raise Exception(\"Execution is taking too long due to too much resource usage and the script was stopped.\")\n",
    "            # Register the signal function handler\n",
    "            signal.signal(signal.SIGALRM, handler)\n",
    "            # Define a time limit\n",
    "            time_limit = 3600  # 1 hour\n",
    "            signal.alarm(time_limit)\n",
    "            try:\n",
    "\n",
    "                # Save model name to a variable\n",
    "                model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "                # Select tokenizer\n",
    "                tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "                # Select model\n",
    "                model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "                # Input the text sequence we want distilBERT to classify into the tokenizer\n",
    "                # Example article to get text from: https://www.forbes.com/sites/dereksaul/2024/02/12/nvidia-is-now-more-valuable-than-amazon-and-google/\n",
    "                tokenized_input = tokenizer(user_input, return_tensors=\"pt\")\n",
    "                # Tokenizing: Splitting text into tokens (words, subwords, characters) and then mapping each token to a unique integer (its ID in the model’s vocabulary)\n",
    "                # Output: PyTorch tensor where each element is the ID of a token in the input text\n",
    "\n",
    "                # Quantize the model for optimized resource usage\n",
    "                quantized_model = torch.quantization.quantize_dynamic(\n",
    "                    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    "                )\n",
    "\n",
    "                # Get the quantized model's output (as raw logits)\n",
    "                output = quantized_model(**tokenized_input)\n",
    "                # The model function takes the tensor of token IDs and feeds it through the model \n",
    "                # The model outputs logits (raw, unnormalized scores) for each class in the model’s classification task\n",
    "                # Output logits are also stored in a PyTorch tensor\n",
    "                # If quantization is not needed, change quantized_model to model\n",
    "\n",
    "                # Get the probabilities by applying softmax function to logits\n",
    "                probs = torch.nn.functional.softmax(output.logits, dim=-1)\n",
    "                # The softmax function converts the logits into probabilities (easier to interpret) \n",
    "                # Output is a tensor of probabilities\n",
    "\n",
    "                # Get the predicted class\n",
    "                predicted_class = torch.argmax(probs)\n",
    "                # Argmax function takes the tensor of probabilities and finds the index (class) with the highest probability\n",
    "                # This is the model’s predicted class for the input text\n",
    "\n",
    "                # Initialize LIME text explainer\n",
    "                explainer = LimeTextExplainer(class_names=[\"Negative\", \"Neutral\", \"Positive\"])\n",
    "                # Define a prediction function\n",
    "                def predictor(texts):\n",
    "                    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "                    outputs = quantized_model(**inputs)\n",
    "                # Change quantized_model to model if quantization is not needed!\n",
    "                    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "                    return probs.detach().numpy()\n",
    "                # Explain the model's prediction\n",
    "                explanation = explainer.explain_instance(user_input, \n",
    "                                                        predictor, \n",
    "                                                        num_features=6, \n",
    "                                                        labels=(predicted_class.item(),))\n",
    "                # Print the explanation\n",
    "                print(explanation)\n",
    "\n",
    "            # Continuation of the emergency termination handler\n",
    "            except Exception as exc:\n",
    "                print(exc)\n",
    "\n",
    "            finally:\n",
    "                # Stop resource monitoring when the main script stops, whether by design or exception\n",
    "                stop_monitoring = True\n",
    "                monitor_thread.join()\n",
    "\n",
    "            # Prepare for graphical display of LIME explainer results\n",
    "            # Get the features and their weights (from the user's input, as weighted by finBERT)\n",
    "            features = explanation.as_list()\n",
    "            # Separate the feature names and weights\n",
    "            feature_names = [feature[0] for feature in features]\n",
    "            feature_weights = [feature[1] for feature in features]\n",
    "            # Create a new figure via matplotlib library\n",
    "            fig, ax = plt.subplots(figsize=(5, 4))\n",
    "            # Plot the feature weights as a bar chart\n",
    "            ax.bar(feature_names, feature_weights)\n",
    "            # Display results in the notebook cell output\n",
    "            display(fig)\n",
    "            plt.close(fig)\n",
    "        \n",
    "        # Catch any/all exceptions from trying to run the model or explainer\n",
    "        except Exception as exc:\n",
    "            print(f\"Error (catching exception related to model execution or explanation generation): {exc}\")\n",
    "    # Error for when there is no user-entered text       \n",
    "    else:\n",
    "        print(\"Missing input!\", \"Please enter a text sequence for the model to classify.\")\n",
    "\n",
    "# Finally call the function to run everything\n",
    "run_model()\n",
    "\n",
    "# Conclude execution time measuring\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Script execution time: {elapsed_time} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
